import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

# Define the sequence
sequence = [1, 3, 2, 3, 4, 3, 8]

# Hyperparameters
num_layers = 6
input_size = 1
hidden_size = 3
output_size = max(sequence) + 1  # Adjust the output size to match the maximum value in the sequence + 1
learning_rate = 0.01
num_iterations = 5

# Define weights and biases for each layer
weights = []
biases = []
for i in range(num_layers):
    weights.append(tf.Variable(tf.random.normal([hidden_size, hidden_size])))
    biases.append(tf.Variable(tf.zeros([hidden_size])))

# Define input and target placeholders
inputs = tf.compat.v1.placeholder(tf.float32, [None, input_size])
targets = tf.compat.v1.placeholder(tf.int32, [None])

# Build the RNN
rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(hidden_size)

# Initialize hidden state
hidden_state = tf.zeros([1, hidden_size])

# Iterate through layers
for i in range(num_layers):
    output, new_hidden_state = rnn_cell(inputs, hidden_state)
    
    # Calculate output at each layer using softmax
    output = tf.nn.softmax(tf.matmul(output, weights[i]) + biases[i])
    
    # Update hidden state
    hidden_state = new_hidden_state

# Define loss function and optimizer
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=targets))
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)

# Training
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for iteration in range(num_iterations):
        # Reset hidden state for each iteration
        hidden_state_val = np.zeros([1, hidden_size])
        
        for i in range(len(sequence) - 1):
            input_val = np.array([[sequence[i]]])
            target_val = np.array([sequence[i+1]])
            
            # Train the network
            _, loss_val = sess.run([optimizer, loss], feed_dict={inputs: input_val, targets: target_val})
    
    # Print weights and biases for each layer only for the final iteration
    print("Weights for each layer (Final Iteration):")
    for i in range(num_layers):
        weights_val, biases_val = sess.run([weights[i], biases[i]])
        print(f"Layer {i+1}:")
        print("Weights:")
        print(weights_val)
        print("Biases:")
        print(biases_val)
        print()

